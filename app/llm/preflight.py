# -*- coding: utf-8 -*-
from __future__ import annotations

import httpx
from loguru import logger

from .endpoints import (
    LLMMode,
    build_gemini_native_models_url,
    build_gemini_openai_models_url,
    build_openai_models_url,
)
from .http import LLMHTTPError, response_json_checked


async def preflight_llm(
    *,
    mode: LLMMode,
    base_url: str,
    api_key: str,
    timeout_seconds: float = 15.0,
) -> None:
    """
    å¯åŠ¨ preflight/healthcheckï¼š
    - openai: GET {base_url}/models
    - gemini_native: GET {root}/v1beta/modelsï¼ˆè‹¥ base_url å·²å« /v1beta åˆ™ä¸é‡å¤æ·»åŠ ï¼‰
    - gemini_openai: GET {root}/v1beta/openai/modelsï¼ˆè‹¥ base_url å·²å«åˆ™ä¸é‡å¤æ·»åŠ ï¼‰

    è¦æ±‚è¿”å› JSONï¼Œå¦åˆ™ç›´æ¥å¤±è´¥å¹¶ç»™å‡ºæ¸…æ™°é”™è¯¯ï¼ˆå«å“åº”ç‰‡æ®µï¼‰ã€‚
    """
    if not base_url:
        raise ValueError("LLM_BASE_URL ä¸èƒ½ä¸ºç©º")
    if not api_key:
        raise ValueError("LLM API key ä¸èƒ½ä¸ºç©ºï¼ˆè¯·é…ç½® GEMINI_API_KEYï¼‰")

    if mode == "openai":
        url = build_openai_models_url(base_url)
        headers = {"Authorization": f"Bearer {api_key}"}
    elif mode == "gemini_native":
        url = build_gemini_native_models_url(base_url)
        headers = {"x-goog-api-key": api_key}
    elif mode == "gemini_openai":
        url = build_gemini_openai_models_url(base_url)
        headers = {"Authorization": f"Bearer {api_key}"}
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ LLM_MODE: {mode}")

    logger.info(f"ğŸ” LLM preflight | mode={mode} url={url}")

    try:
        async with httpx.AsyncClient(timeout=httpx.Timeout(timeout_seconds), follow_redirects=True) as client:
            resp = await client.get(url, headers=headers)
        _ = response_json_checked(resp, context="preflight")
    except (LLMHTTPError, httpx.HTTPError) as e:
        logger.error(f"âŒ LLM preflight å¤±è´¥ | mode={mode} url={url} err={e}")
        raise

    logger.success(f"âœ… LLM preflight OK | mode={mode}")


